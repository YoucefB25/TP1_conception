{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Private Synopsis\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/opendp/smartnoise-sdk/blob/main/sql/samples/Synopsis.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/opendp/smartnoise-sdk/blob/main/sql/samples/Synopsis.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://www.kaggle.com/notebooks/welcome?src=https://github.com/opendp/smartnoise-sdk/blob/main/sql/samples/Synopsis.ipynb\">\n",
        "      <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAABJ0lEQVR4AezUgUYEURSH8c3IWkDIPkEI+wwhK/sMPUCSAYGQEBCwIBACyYKEIIAkrCQQkmSMJFnJYHD6cCJ/CdaZWQx+hjuX881wb8vM5uo0+wGtg8sxMnddR8AE5t6agCagCZj5APYkP6IDdF8fxzjFCUZIkYQHsD7AM+yXEnvhf4C1NR2OAvvohAb4b3/S4diFDw8K8OGPMvwLO2iHngKeKzocn9jGfOgxRIoHGT5BKsNDAkp8yPB3bCCJv4j+diRfXnlAhn5tAe4Oy1UFvOAMpayfYzE6oMA6FjCCiaHcAaEX0RLGMAncCg2Qd6vIJeIVg8gAfb+JQiLu0asqoI0hTFygGx7ge7o+0MQhOtMG3CJ3N//s6+EKOTJ/fg+kVxmf+aO9YwBrLxFeFqDOPAAAAABJRU5ErkJggg==\" alt=\"Kaggle logo\">\n",
        "      Run on Kaggle\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ],
      "metadata": {
        "id": "9IkPPqiAySkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "ekg_RNWsySkq",
        "outputId": "6814022e-c9f7-4a0b-d058-4415a3a707f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.10.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (24.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.12.14)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.10.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.10.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 w3lib-2.3.1 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy.spiders import CrawlSpider,Rule\n",
        "from scrapy.linkextractors import LinkExtractor"
      ],
      "metadata": {
        "trusted": true,
        "id": "2DAI_KoHySkv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrawlingSpider (CrawlSpider):\n",
        "  name=\"mycrawler\"\n",
        "  allowed_domains=[\"toscrape.com\"]\n",
        "  start_urls=[\"http://books.toscrape.com/\"]\n",
        "\n",
        "  rules=(\n",
        "      Rule(LinkExtractor(allow=\"catalogue/category\")),\n",
        "      Rule(LinkExtractor(allow=\"catalogue\",deny=\"category\"),callback=\"parse_item\")\n",
        "  )\n",
        "\n",
        "  def parse_item(self,response):\n",
        "    yield{\n",
        "        \"title\"=response.css(\"h1::text\").get(),\n",
        "        \"price\"=response.css(\".price_color::text\").get(),\n",
        "        \"availability\"=response.css(\".availability::text\").get()\n",
        "    }"
      ],
      "metadata": {
        "id": "UV8zitJVy4Mh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Data"
      ],
      "metadata": {
        "id": "GQvPVrNVySkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "url_file = 'https://raw.githubusercontent.com/opendifferentialprivacy/dp-test-datasets/master/data/PUMS_california_demographics/data.csv'\n",
        "local_file = Path('PUMS_large.csv')\n",
        "\n",
        "!wget -nc {url_file} -O {str(local_file)}"
      ],
      "metadata": {
        "trusted": true,
        "id": "cXj4oJRdySky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data\n",
        "\n",
        "First we load the California demographic data into a Spark `DataFrame`.  We let Spark infer the column names and types, then clean things up a bit."
      ],
      "metadata": {
        "id": "3JowLYfhySk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import FloatType, BooleanType\n",
        "\n",
        "filepath = str(local_file)\n",
        "pums = spark.read.load(filepath, format=\"csv\", sep=\",\",inferSchema=\"true\", header=\"true\")\n",
        "\n",
        "pums = pums.withColumnRenamed(\"_c0\", \"PersonID\")\n",
        "\n",
        "pums = pums.withColumn(\"income\", col(\"income\").cast(FloatType()))\n",
        "pums = pums.withColumn(\"latino\", col(\"latino\").cast(BooleanType()))\n",
        "pums = pums.withColumn(\"black\", col(\"black\").cast(BooleanType()))\n",
        "pums = pums.withColumn(\"asian\", col(\"asian\").cast(BooleanType()))\n",
        "pums = pums.withColumn(\"married\", col(\"married\").cast(BooleanType()))\n",
        "\n",
        "pums.show(5)\n",
        "print(\"There are {0} individuals in the data\".format(pums.count()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "WUdsuoCVySk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from snsql import *\n",
        "from pathlib import Path\n",
        "\n",
        "metadata_path = Path(\"./PUMS_large.yaml\")\n",
        "metadata_string = \"\"\"PUMS:\n",
        "  PUMS:\n",
        "    PUMS_large:\n",
        "      clamp_counts: False\n",
        "      censor_dims: False\n",
        "      PersonID:\n",
        "        type: int\n",
        "        private_id: True\n",
        "      state:\n",
        "        type: int\n",
        "      puma:\n",
        "        type: string\n",
        "      sex:\n",
        "        type: string\n",
        "      age:\n",
        "        lower: 0\n",
        "        upper: 100\n",
        "        type: int\n",
        "      educ:\n",
        "        type: string\n",
        "      income:\n",
        "        lower: 0\n",
        "        upper: 500000\n",
        "        type: float\n",
        "      latino:\n",
        "        type: boolean\n",
        "      black:\n",
        "        type: boolean\n",
        "      asian:\n",
        "        type: boolean\n",
        "      married:\n",
        "        type: boolean\n",
        "\"\"\"\n",
        "with open(metadata_path, \"w+\") as f:\n",
        "    f.write(metadata_string)\n",
        "\n",
        "pums.createOrReplaceTempView(\"PUMS_large\")\n",
        "\n",
        "private_reader = from_connection(\n",
        "    spark,\n",
        "    metadata=str(metadata_path),\n",
        "    privacy=Privacy(epsilon=3.0, delta=1/1_000_000)\n",
        ")\n",
        "private_reader.reader.compare.search_path = [\"PUMS\"]\n",
        "\n",
        "\n",
        "res = private_reader.execute('SELECT COUNT(*) FROM PUMS_large')\n",
        "res.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ygbYToSoySk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in the data represents a unique individual.\n",
        "\n",
        "# Get Exact Values for Comparison\n",
        "\n",
        "The `private_reader` wraps an existing SparkSQL session and applies differential privacy.  We can access the underlying reader to get exact results with no differential privacy.  This is useful for comparing utility.  For example, we can compute the average income for individuals in the database."
      ],
      "metadata": {
        "id": "991r8VZ9ySk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'SELECT COUNT(*) AS count, AVG(income) FROM PUMS_large'\n",
        "\n",
        "reader = private_reader.reader # the underlying connection to Spark SQL\n",
        "\n",
        "res = reader.execute(query)\n",
        "res.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Pb-EafHxySk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Exact Synopsis\n",
        "\n",
        "We can use the `SparkReader` to create a synopsis file that calculates some metrics grouped by the dimensions in the data.  We can then load the synopsis into an Excel spreadsheet to use in a Pivot Table, or query the synopsis from Pandas."
      ],
      "metadata": {
        "id": "atGNVhSfySk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'SELECT sex, age, educ, married, latino, black, asian, COUNT(*) AS n, AVG(income) AS income FROM PUMS_large GROUP BY sex, age, educ, married, latino, black, asian ORDER BY n DESC'\n",
        "\n",
        "synopsis = private_reader.reader.execute(query)\n",
        "synopsis.show(15)\n",
        "print(\"{0} distinct dimensions\".format(synopsis.count()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EW-kVEJBySk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have far fewer rows, but we can still recover the exact values.  For example, the average income queried from our synopsis exactly matches the average income we queried above:"
      ],
      "metadata": {
        "id": "JFZB5wiIySk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synopsis.createOrReplaceTempView(\"Synopsis\")\n",
        "\n",
        "res = reader.execute(\"SELECT SUM(n) AS count, SUM(income * n) / SUM(n) AS avg_income FROM Synopsis\")\n",
        "res.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "j4uHO6KcySk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have gone from 1.2 million rows to approximately 20,000 rows, so each row in our synopsis no longer represents an individual.  However, we have still not attempted to use any differential privacy, so our synopsis is not private.  For example, there are several dimensions in our synopsis which uniquely identify individuals."
      ],
      "metadata": {
        "id": "VSawamJWySk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader.execute(\"SELECT * FROM Synopsis WHERE n <= 1\").show(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dpblMErUySk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, cells with exact counts > 1 can still leak privacy.  To protect against these and other attacks, let's make our synopsis private.\n",
        "\n",
        "# Generate Private Synopsis\n",
        "\n",
        "To generate a private synopsis, we use the same query we used to create the exact synopsis, but we use a `PrivateReader`, which transparently adds differential privacy."
      ],
      "metadata": {
        "id": "joh9DgUvySk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DecimalType\n",
        "\n",
        "private_synopsis = private_reader.execute(query) # using same query from before\n",
        "private_synopsis = private_synopsis.withColumn('income', private_synopsis.income.cast(DecimalType(18, 2)))\n",
        "private_synopsis.show(15)\n",
        "print(\"{0} distinct dimensions\".format(private_synopsis.count()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "uOV6XjYZySk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the `PrivateReader` transparently adds noise, you will notice that the numbers change each time you run the cell above, sometimes even returning negative counts or negative incomes.  However, the larger aggregates are still fairly accurate, because the noise is symmetrical:"
      ],
      "metadata": {
        "id": "cg9AfRK_ySk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "private_synopsis.persist().createOrReplaceTempView(\"PrivateSynopsis\")\n",
        "\n",
        "reader.execute(\"SELECT SUM(n) AS count, SUM(income * n) / SUM(n) AS avg_income FROM PrivateSynopsis\").show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "79nJIN-kySk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we call `persist()` when loading the private synopsis into a view.  This is how we ensure that Spark doesn't generate a new synopsis every time we query the synopsis.  The goal of a synopsis is to support many queries from a single generation, and we do not want to pay additional `epsilon` privacy cost every time we use the synopsis.  If we create the synopsis once, we can export to Excel or persist in a view, then query indefinitely without incurring further privacy cost.\n",
        "\n",
        "## PrivateReader Parameters\n",
        "\n",
        "When we created the `PrivateReader` above, we passed in the `epsilon` parameter and wrapped our existing `SparkReader` we created earlier.  The `PrivateReader` simply intercepts calls to `SparkReader` and adds noise calibrated to the requested `epsilon`.  We also passed in some metadata describing the sensitivity of the fields in the data source, loaded from a YAML file.  In particular, the algorithm needed to know that the `income` field ranges between 0 and 500,000, in order to appropriately calibrate the noise:"
      ],
      "metadata": {
        "id": "ur5ZrNbsySk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snsql\n",
        "meta = snsql.metadata.Metadata.from_file(metadata_path)\n",
        "print(meta)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OxyWkc1VySk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also notice that the YAML file refers to the PUMS_large table with the prefix PUMS, which is a convention used in the SQL-92 specification allowing tables and views to be grouped together.  Although we are only querying a single source table here, the readers support querying over multiple tables.  Since our query does not specify the full disambiguated table name, we tell our reader to treat PUMS as a default namespace by specifying `private.reader.compare.search_path`.\n",
        "\n",
        "You can read more about the other `PrivateReader` options [here](https://opendifferentialprivacy.github.io/smartnoise-samples/docs/api/system/sql/private_reader.html#opendp.smartnoise.sql.private_reader.PrivateReaderOptions)\n",
        "\n",
        "# Censoring Infrequent Dimensions\n",
        "\n",
        "One option worth exploring further is the `censor_dims` option we set to `False` above.  Recall that the number of distinct dimensions in our private synopsis was exactly the same as the number of distinct dimesions in our exact synopsis.  In our exact synopsis, the existence of dimensions with exactly one member constituted a privacy leak.  Since we have added noise, dimensions with fewer than 2 members are significantly less useful:\n"
      ],
      "metadata": {
        "id": "AeneX3z8ySlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader.execute(\"SELECT * FROM PrivateSynopsis WHERE n <= 1\").show(8)"
      ],
      "metadata": {
        "trusted": true,
        "id": "odef-7hUySlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, it is possible that the mere existence of a unique dimension combination in the data set would constitute a privacy failure.  For example, if this data represented people with a sensitive medical condition, mere membership would sensitive.  If we want to protect the queries further, we can tell the system to hide infrequent dimensions, adhering to epsilon, delta differential privacy."
      ],
      "metadata": {
        "id": "ehjAJrG1ySlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(query)\n",
        "delta = 1/1_200_000\n",
        "\n",
        "meta[\"PUMS.PUMS_large\"].censor_dims = True\n",
        "\n",
        "private_reader = from_connection(\n",
        "    spark,\n",
        "    metadata=meta,\n",
        "    privacy=Privacy(epsilon=3.0, delta=delta)\n",
        ")\n",
        "private_reader.reader\n",
        "private_reader.reader.compare.search_path = [\"PUMS\"]\n",
        "\n",
        "\n",
        "private_synopsis = private_reader.execute(query)\n",
        "print(\"{0} distinct dimensions\".format(private_synopsis.count()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EMT7BZZyySlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "private_synopsis.persist().createOrReplaceTempView(\"PrivateSynopsis\")\n",
        "\n",
        "reader.execute(\"SELECT SUM(n) AS count, SUM(income * n) / SUM(n) AS avg_income FROM PrivateSynopsis\").show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3KWjoZhqySlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the outputs, you can see the private synopsis still computes an accurate average income, but we are missing about 6,500 dimensions, representing about 12,000 individuals.  It may be desirable to leave the synopsis like this, to indicate that some individuals have been dropped for privacy reasons.  In some settings, however, this is undesirable, because aggregate counts will be biased downward.  To resolve this, we can add an `other` dimension that recaptures the dropped dimension.\n",
        "\n",
        "## Recovering Infrequent Dimensions"
      ],
      "metadata": {
        "id": "y7vZ8o8aySlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the dimensions\n",
        "other = 'SELECT DISTINCT sex, age, educ, married, latino, black, asian FROM PUMS_large EXCEPT (SELECT DISTINCT sex, age, educ, married, latino, black, asian FROM PrivateSynopsis)'\n",
        "other_dims = reader.execute(other)\n",
        "other_dims.persist().createOrReplaceTempView(\"OtherDims\")\n",
        "print(\"Combining {0} distinct dimensions that were dropped.\".format(other_dims.count()))\n",
        "\n",
        "# get rows that match censored dimensions\n",
        "filtered = 'SELECT t1.* FROM PUMS_large t1 LEFT SEMI JOIN OtherDims t2 ON ( t1.sex = t2.sex AND t1.age = t2.age AND t1.educ = t2.educ AND t1.married = t2.married AND t1.latino = t2.latino AND t1.black = t2.black AND t1.asian = t2.asian)'\n",
        "filtered_pums = reader.execute(filtered)\n",
        "filtered_pums.persist().createOrReplaceTempView(\"PUMS_censored\")\n",
        "print(\"Selecting {0} records from the dimensions that were censored\".format(filtered_pums.count()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "wepR3PzQySlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a table, `PUMS_censored`, which has all the records which were censored from our private synopsis.  We can create a differentially private result, treating all of our censored dimensions as a single `other` dimension.  To query these remaining records, we need metadata that describes the new table, `PUMS_censored`.  Since this has the same schema as `PUMS_large`, we can grab the original schema and make a copy for the new table."
      ],
      "metadata": {
        "id": "V2hLVhytySlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "pc = copy.copy(meta.m_tables['PUMS.PUMS_large'])\n",
        "pc.name = 'PUMS_censored'\n",
        "meta.m_tables['PUMS.PUMS_censored'] = pc"
      ],
      "metadata": {
        "trusted": true,
        "id": "kWg-4wQlySlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_single = 'SELECT COUNT(*) AS n, AVG(income) AS income FROM PUMS_censored'\n",
        "missing_dim = private_reader.execute(query_single).persist()\n",
        "missing_dim.createOrReplaceTempView(\"Censored\")\n",
        "missing_dim.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tsXGzN0EySlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noisy Values for Missing Dimensions\n",
        "\n",
        "Another option is to create a private synopsis for all possible dimension values, where missing values will be set to NULL, which will result in zero counts.  These zero counts will result in zero values.  This approach is not suitable in settings where rare dimensions are private, such as surnames, or when the cross product of all dimensions is very large.  In this case, however, the distinct dimension members are not private, and the cross product is not large."
      ],
      "metadata": {
        "id": "UgV-RjblySlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alldims = 'SELECT s.*, a.*, e.*, m.*, l.*, b.*, asi.* \\\n",
        "    FROM (SELECT DISTINCT sex FROM PUMS_large) s \\\n",
        "    CROSS JOIN (SELECT DISTINCT age FROM PUMS_large) a \\\n",
        "    CROSS JOIN (SELECT DISTINCT educ FROM PUMS_large) e \\\n",
        "    CROSS JOIN (SELECT DISTINCT married FROM PUMS_large) m \\\n",
        "    CROSS JOIN (SELECT DISTINCT latino FROM PUMS_large) l \\\n",
        "    CROSS JOIN (SELECT DISTINCT black FROM PUMS_large) b \\\n",
        "    CROSS JOIN (SELECT DISTINCT asian FROM PUMS_large) asi'\n",
        "\n",
        "all_dims = reader.execute(alldims)\n",
        "all_dims.persist().createOrReplaceTempView(\"AllDims\")\n",
        "\n",
        "print(\"Including empty dimensions, there are {0} total dimensions\".format(all_dims.count()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "OmFq5_uwySlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the empty dimensions increases our total number of dimensions by about 16,000."
      ],
      "metadata": {
        "id": "FXYxxGW0ySlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined = 'SELECT p.PersonID, p.state, p.puma, d.sex, d.age, d.educ, d.latino, d.black, d.asian, d.married, p.income \\\n",
        "    FROM AllDims d LEFT OUTER JOIN PUMS_large p ON \\\n",
        "        d.sex = p.sex AND \\\n",
        "        d.age = p.age AND \\\n",
        "        d.educ = p.educ AND \\\n",
        "        d.latino = p.latino AND \\\n",
        "        d.black = p.black AND \\\n",
        "        d.asian = p.asian AND \\\n",
        "        d.married = p.married'\n",
        "\n",
        "joined_pums = reader.execute(joined).persist()\n",
        "joined_pums.createOrReplaceTempView(\"PUMS_joined\")\n",
        "print(\"There are {0} rows, including empty dimensions\".format(joined_pums.count()))\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "mgNv0-ibySlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc = copy.copy(meta.m_tables['PUMS.PUMS_large'])\n",
        "pc.name = 'PUMS_joined'\n",
        "meta.m_tables['PUMS.PUMS_joined'] = pc"
      ],
      "metadata": {
        "trusted": true,
        "id": "CSmYUxAVySlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[\"PUMS.PUMS_large\"].censor_dims = False\n",
        "meta[\"PUMS.PUMS_large\"].clamp_counts = False\n",
        "meta[\"PUMS.PUMS_large\"].row_privacy = True\n",
        "\n",
        "\n",
        "q = 'SELECT sex, age, educ, married, latino, black, asian, COUNT(*) AS n, AVG(income) AS income FROM PUMS_joined GROUP BY sex, age, educ, married, latino, black, asian'\n",
        "\n",
        "priv2 = private_reader.execute(q).persist()\n",
        "priv2.createOrReplaceTempView(\"PrivateSynopsis2\")\n",
        "print(\"The new private synopsis has {0} dimensions\".format(priv2.count()))\n",
        "reader.execute(\"SELECT SUM(n) AS count, SUM(income * n) / SUM(n) AS avg_income FROM PrivateSynopsis2\").show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "dzMGSXB3ySlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}